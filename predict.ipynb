{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce3ff5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0273039dfa6a4d7696588e032221b18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\1.apps\\python\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PHONG\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb67f6e6b29f42aba8beb3637f804911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06540a5453e45bbbb7641d37c9a4420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Initialize the tokenizer for our chosen model\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdae819",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_finqa(examples):\n",
    "    \"\"\"\n",
    "    Prepares the FinQA dataset for a T5 model.\n",
    "    The input to the model will be a single string containing the question and context.\n",
    "    The output will be the reasoning program.\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    for i in range(len(examples['pre_text'])):\n",
    "        # Construct the input string\n",
    "        question = examples['qa'][i]['question']\n",
    "\n",
    "        # Linearize the table using pandas for a clean string representation\n",
    "        table_data = examples['table'][i]\n",
    "        if table_data:\n",
    "            df = pd.DataFrame(table_data[1:], columns=table_data[0])\n",
    "            table_str = df.to_string()\n",
    "        else:\n",
    "            table_str = \"\"\n",
    "\n",
    "        # Combine all parts for the model's input\n",
    "        input_text = f\"question: {question} context: {examples['pre_text'][i]} {table_str} {examples['post_text'][i]}\"\n",
    "        inputs.append(input_text)\n",
    "\n",
    "        # The target for the model is the reasoning program\n",
    "        program = examples['qa'][i]['program']\n",
    "        targets.append(program)\n",
    "\n",
    "    # Tokenize the processed inputs and targets\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5c12c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset from Hugging Face\n",
    "# Using a pre-split version for convenience\n",
    "try:\n",
    "    finqa_dataset = load_dataset(\"dreamerdeo/finqa\")\n",
    "    # Using smaller splits for a quick demonstration\n",
    "    # For a real run, use the full dataset\n",
    "    train_dataset = finqa_dataset['train'].select(range(1000)) # Use more data for a real run\n",
    "    validation_dataset = finqa_dataset['validation'].select(range(200))\n",
    "    test_dataset = finqa_dataset['test'].select(range(200))\n",
    "    \n",
    "    small_finqa_dataset = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'validation': validation_dataset,\n",
    "        'test': test_dataset\n",
    "    })\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to load dataset. Make sure you are connected to the internet. Error: {e}\")\n",
    "    # You would exit or handle this error in a real script\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8a9187",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Apply the preprocessing function to the entire dataset\n",
    "tokenized_datasets = small_finqa_dataset.map(preprocess_finqa, batched=True)\n",
    "\n",
    "print(\"Data preparation complete. Example of tokenized input:\")\n",
    "print(tokenized_datasets['train'][0].keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
